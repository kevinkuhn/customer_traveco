{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 05: Exploratory Data Analysis (EDA)\n",
    "\n",
    "**Purpose**: Deep dive into monthly aggregated data to understand patterns, seasonality, and trends\n",
    "\n",
    "**Input**: `data/processed/monthly_aggregated.csv`\n",
    "\n",
    "**Output**: \n",
    "- Seasonal decomposition results\n",
    "- Correlation analysis\n",
    "- Pattern identification\n",
    "- Insights for model selection\n",
    "\n",
    "**Author**: Kevin Kuhn  \n",
    "**Date**: 2025-10-17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "Analysis timestamp: 2025-10-17 23:02:52\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from scipy import stats\n",
    "from datetime import datetime\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from utils.traveco_utils import ConfigLoader, load_processed_data\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Analysis timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Configuration and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 19 rows from: ../data/processed/monthly_aggregated.csv\n",
      "\n",
      "Available columns: ['year_month', 'Id.Dispostelle', 'total_orders', 'total_distance_km', 'avg_distance_km', 'median_distance_km', 'external_driver_orders', 'internal_driver_orders', 'pickup_orders', 'delivery_orders', 'date', 'year', 'month', 'quarter', 'month_name', 'split']\n",
      "\n",
      "First few rows:\n",
      "  year_month               Id.Dispostelle  total_orders  total_distance_km  \\\n",
      "0    2025-06                            -          3544               54.0   \n",
      "1    2025-06       1000_TRP_Lager Nebikon           508            41606.0   \n",
      "2    2025-06              14_TRP_Oberbipp         31080          2046293.0   \n",
      "3    2025-06            15_TRP_Intermodal           894            65662.0   \n",
      "4    2025-06  16_TRP_Herzogenbuchsee (DP)          4312           277401.0   \n",
      "\n",
      "   avg_distance_km  median_distance_km  external_driver_orders  \\\n",
      "0        18.000000                24.0                       0   \n",
      "1        82.063116                83.0                       0   \n",
      "2        65.839543                58.0                    8578   \n",
      "3        80.468137                54.0                     290   \n",
      "4        64.332328                55.0                     262   \n",
      "\n",
      "   internal_driver_orders  pickup_orders  delivery_orders        date  year  \\\n",
      "0                    2375              0             3543  2025-06-01  2025   \n",
      "1                       0              0              508  2025-06-01  2025   \n",
      "2                   22502           1254            29826  2025-06-01  2025   \n",
      "3                     604            374              520  2025-06-01  2025   \n",
      "4                    4050              0             4312  2025-06-01  2025   \n",
      "\n",
      "   month  quarter month_name  split  \n",
      "0      6        2       June  train  \n",
      "1      6        2       June  train  \n",
      "2      6        2       June  train  \n",
      "3      6        2       June  train  \n",
      "4      6        2       June  train  \n",
      "\n",
      "Data shape: (19, 16)\n",
      "Date range: 2025-06-01 00:00:00 to 2025-06-01 00:00:00\n",
      "Unique dates: 1\n",
      "\n",
      "⚠️  No branch column found - data may already be aggregated to company level\n"
     ]
    }
   ],
   "source": [
    "# Load configuration (use relative path from notebooks directory)\n",
    "config = ConfigLoader('../config/config.yaml')\n",
    "\n",
    "# Load monthly aggregated data\n",
    "df = load_processed_data('monthly_aggregated.csv', config)\n",
    "\n",
    "# First, let's see what columns we have\n",
    "print(f\"\\nAvailable columns: {list(df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# Convert year_month to datetime (handle different possible column names)\n",
    "if 'year_month' in df.columns:\n",
    "    df['year_month'] = pd.to_datetime(df['year_month'])\n",
    "elif 'date' in df.columns:\n",
    "    df['year_month'] = pd.to_datetime(df['date'])\n",
    "else:\n",
    "    print(f\"⚠️  Warning: No date column found. Available columns: {list(df.columns)}\")\n",
    "\n",
    "print(f\"\\nData shape: {df.shape}\")\n",
    "\n",
    "if 'year_month' in df.columns:\n",
    "    print(f\"Date range: {df['year_month'].min()} to {df['year_month'].max()}\")\n",
    "    print(f\"Unique dates: {df['year_month'].nunique()}\")\n",
    "\n",
    "# Check for branch column (could be named differently)\n",
    "branch_col = None\n",
    "for possible_name in ['Niederlassung', 'branch', 'Branch', 'branch_name']:\n",
    "    if possible_name in df.columns:\n",
    "        branch_col = possible_name\n",
    "        break\n",
    "\n",
    "if branch_col:\n",
    "    print(f\"\\nBranches (using '{branch_col}'): {df[branch_col].nunique()}\")\n",
    "else:\n",
    "    print(f\"\\n⚠️  No branch column found - data may already be aggregated to company level\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample Data ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year_month</th>\n",
       "      <th>Id.Dispostelle</th>\n",
       "      <th>total_orders</th>\n",
       "      <th>total_distance_km</th>\n",
       "      <th>avg_distance_km</th>\n",
       "      <th>median_distance_km</th>\n",
       "      <th>external_driver_orders</th>\n",
       "      <th>internal_driver_orders</th>\n",
       "      <th>pickup_orders</th>\n",
       "      <th>delivery_orders</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>quarter</th>\n",
       "      <th>month_name</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-06-01</td>\n",
       "      <td>-</td>\n",
       "      <td>3544</td>\n",
       "      <td>54.0</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2375</td>\n",
       "      <td>0</td>\n",
       "      <td>3543</td>\n",
       "      <td>2025-06-01</td>\n",
       "      <td>2025</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>June</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-06-01</td>\n",
       "      <td>1000_TRP_Lager Nebikon</td>\n",
       "      <td>508</td>\n",
       "      <td>41606.0</td>\n",
       "      <td>82.063116</td>\n",
       "      <td>83.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>508</td>\n",
       "      <td>2025-06-01</td>\n",
       "      <td>2025</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>June</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-06-01</td>\n",
       "      <td>14_TRP_Oberbipp</td>\n",
       "      <td>31080</td>\n",
       "      <td>2046293.0</td>\n",
       "      <td>65.839543</td>\n",
       "      <td>58.0</td>\n",
       "      <td>8578</td>\n",
       "      <td>22502</td>\n",
       "      <td>1254</td>\n",
       "      <td>29826</td>\n",
       "      <td>2025-06-01</td>\n",
       "      <td>2025</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>June</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-06-01</td>\n",
       "      <td>15_TRP_Intermodal</td>\n",
       "      <td>894</td>\n",
       "      <td>65662.0</td>\n",
       "      <td>80.468137</td>\n",
       "      <td>54.0</td>\n",
       "      <td>290</td>\n",
       "      <td>604</td>\n",
       "      <td>374</td>\n",
       "      <td>520</td>\n",
       "      <td>2025-06-01</td>\n",
       "      <td>2025</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>June</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-06-01</td>\n",
       "      <td>16_TRP_Herzogenbuchsee (DP)</td>\n",
       "      <td>4312</td>\n",
       "      <td>277401.0</td>\n",
       "      <td>64.332328</td>\n",
       "      <td>55.0</td>\n",
       "      <td>262</td>\n",
       "      <td>4050</td>\n",
       "      <td>0</td>\n",
       "      <td>4312</td>\n",
       "      <td>2025-06-01</td>\n",
       "      <td>2025</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>June</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-06-01</td>\n",
       "      <td>18_TRP_Puidoux (DP)</td>\n",
       "      <td>888</td>\n",
       "      <td>51974.0</td>\n",
       "      <td>58.529279</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0</td>\n",
       "      <td>888</td>\n",
       "      <td>0</td>\n",
       "      <td>888</td>\n",
       "      <td>2025-06-01</td>\n",
       "      <td>2025</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>June</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-06-01</td>\n",
       "      <td>19_TRP_Sierre</td>\n",
       "      <td>8582</td>\n",
       "      <td>474577.0</td>\n",
       "      <td>55.299114</td>\n",
       "      <td>34.0</td>\n",
       "      <td>81</td>\n",
       "      <td>8501</td>\n",
       "      <td>2910</td>\n",
       "      <td>5672</td>\n",
       "      <td>2025-06-01</td>\n",
       "      <td>2025</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>June</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-06-01</td>\n",
       "      <td>1_B&amp;T_1100 Ohringen</td>\n",
       "      <td>5295</td>\n",
       "      <td>135804.0</td>\n",
       "      <td>25.647592</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3722</td>\n",
       "      <td>0</td>\n",
       "      <td>5295</td>\n",
       "      <td>2025-06-01</td>\n",
       "      <td>2025</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>June</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-06-01</td>\n",
       "      <td>1_TRP_Lahr</td>\n",
       "      <td>2330</td>\n",
       "      <td>9038.0</td>\n",
       "      <td>50.775281</td>\n",
       "      <td>43.0</td>\n",
       "      <td>2152</td>\n",
       "      <td>178</td>\n",
       "      <td>1488</td>\n",
       "      <td>842</td>\n",
       "      <td>2025-06-01</td>\n",
       "      <td>2025</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>June</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-06-01</td>\n",
       "      <td>2_B&amp;T_1200 Landquart</td>\n",
       "      <td>600</td>\n",
       "      <td>15897.0</td>\n",
       "      <td>26.495000</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>599</td>\n",
       "      <td>0</td>\n",
       "      <td>600</td>\n",
       "      <td>2025-06-01</td>\n",
       "      <td>2025</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>June</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  year_month               Id.Dispostelle  total_orders  total_distance_km  \\\n",
       "0 2025-06-01                            -          3544               54.0   \n",
       "1 2025-06-01       1000_TRP_Lager Nebikon           508            41606.0   \n",
       "2 2025-06-01              14_TRP_Oberbipp         31080          2046293.0   \n",
       "3 2025-06-01            15_TRP_Intermodal           894            65662.0   \n",
       "4 2025-06-01  16_TRP_Herzogenbuchsee (DP)          4312           277401.0   \n",
       "5 2025-06-01          18_TRP_Puidoux (DP)           888            51974.0   \n",
       "6 2025-06-01                19_TRP_Sierre          8582           474577.0   \n",
       "7 2025-06-01          1_B&T_1100 Ohringen          5295           135804.0   \n",
       "8 2025-06-01                   1_TRP_Lahr          2330             9038.0   \n",
       "9 2025-06-01         2_B&T_1200 Landquart           600            15897.0   \n",
       "\n",
       "   avg_distance_km  median_distance_km  external_driver_orders  \\\n",
       "0        18.000000                24.0                       0   \n",
       "1        82.063116                83.0                       0   \n",
       "2        65.839543                58.0                    8578   \n",
       "3        80.468137                54.0                     290   \n",
       "4        64.332328                55.0                     262   \n",
       "5        58.529279                51.0                       0   \n",
       "6        55.299114                34.0                      81   \n",
       "7        25.647592                21.0                       0   \n",
       "8        50.775281                43.0                    2152   \n",
       "9        26.495000                21.0                       0   \n",
       "\n",
       "   internal_driver_orders  pickup_orders  delivery_orders        date  year  \\\n",
       "0                    2375              0             3543  2025-06-01  2025   \n",
       "1                       0              0              508  2025-06-01  2025   \n",
       "2                   22502           1254            29826  2025-06-01  2025   \n",
       "3                     604            374              520  2025-06-01  2025   \n",
       "4                    4050              0             4312  2025-06-01  2025   \n",
       "5                     888              0              888  2025-06-01  2025   \n",
       "6                    8501           2910             5672  2025-06-01  2025   \n",
       "7                    3722              0             5295  2025-06-01  2025   \n",
       "8                     178           1488              842  2025-06-01  2025   \n",
       "9                     599              0              600  2025-06-01  2025   \n",
       "\n",
       "   month  quarter month_name  split  \n",
       "0      6        2       June  train  \n",
       "1      6        2       June  train  \n",
       "2      6        2       June  train  \n",
       "3      6        2       June  train  \n",
       "4      6        2       June  train  \n",
       "5      6        2       June  train  \n",
       "6      6        2       June  train  \n",
       "7      6        2       June  train  \n",
       "8      6        2       June  train  \n",
       "9      6        2       June  train  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Data Types ===\n",
      "year_month                datetime64[ns]\n",
      "Id.Dispostelle                    object\n",
      "total_orders                       int64\n",
      "total_distance_km                float64\n",
      "avg_distance_km                  float64\n",
      "median_distance_km               float64\n",
      "external_driver_orders             int64\n",
      "internal_driver_orders             int64\n",
      "pickup_orders                      int64\n",
      "delivery_orders                    int64\n",
      "date                              object\n",
      "year                               int64\n",
      "month                              int64\n",
      "quarter                            int64\n",
      "month_name                        object\n",
      "split                             object\n",
      "dtype: object\n",
      "\n",
      "=== Summary Statistics ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year_month</th>\n",
       "      <th>total_orders</th>\n",
       "      <th>total_distance_km</th>\n",
       "      <th>avg_distance_km</th>\n",
       "      <th>median_distance_km</th>\n",
       "      <th>external_driver_orders</th>\n",
       "      <th>internal_driver_orders</th>\n",
       "      <th>pickup_orders</th>\n",
       "      <th>delivery_orders</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>quarter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>19</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>1.900000e+01</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2025-06-01 00:00:00</td>\n",
       "      <td>7166.263158</td>\n",
       "      <td>4.317172e+05</td>\n",
       "      <td>55.915842</td>\n",
       "      <td>45.894737</td>\n",
       "      <td>1304.315789</td>\n",
       "      <td>5615.684211</td>\n",
       "      <td>909.736842</td>\n",
       "      <td>6256.473684</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2025-06-01 00:00:00</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.400000e+01</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2025-06-01 00:00:00</td>\n",
       "      <td>781.000000</td>\n",
       "      <td>2.379300e+04</td>\n",
       "      <td>47.239219</td>\n",
       "      <td>35.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>388.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>637.000000</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2025-06-01 00:00:00</td>\n",
       "      <td>2330.000000</td>\n",
       "      <td>1.192610e+05</td>\n",
       "      <td>55.299114</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>1612.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1589.000000</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2025-06-01 00:00:00</td>\n",
       "      <td>6938.500000</td>\n",
       "      <td>3.759890e+05</td>\n",
       "      <td>66.002197</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>1221.000000</td>\n",
       "      <td>6275.500000</td>\n",
       "      <td>1371.000000</td>\n",
       "      <td>5483.500000</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2025-06-01 00:00:00</td>\n",
       "      <td>31080.000000</td>\n",
       "      <td>2.290246e+06</td>\n",
       "      <td>87.617965</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>8578.000000</td>\n",
       "      <td>23373.000000</td>\n",
       "      <td>5344.000000</td>\n",
       "      <td>29826.000000</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10327.643201</td>\n",
       "      <td>7.126653e+05</td>\n",
       "      <td>19.560179</td>\n",
       "      <td>20.736159</td>\n",
       "      <td>2439.802680</td>\n",
       "      <td>8282.104584</td>\n",
       "      <td>1496.157963</td>\n",
       "      <td>9236.427751</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                year_month  total_orders  total_distance_km  avg_distance_km  \\\n",
       "count                   19     19.000000       1.900000e+01        19.000000   \n",
       "mean   2025-06-01 00:00:00   7166.263158       4.317172e+05        55.915842   \n",
       "min    2025-06-01 00:00:00      5.000000       5.400000e+01        18.000000   \n",
       "25%    2025-06-01 00:00:00    781.000000       2.379300e+04        47.239219   \n",
       "50%    2025-06-01 00:00:00   2330.000000       1.192610e+05        55.299114   \n",
       "75%    2025-06-01 00:00:00   6938.500000       3.759890e+05        66.002197   \n",
       "max    2025-06-01 00:00:00  31080.000000       2.290246e+06        87.617965   \n",
       "std                    NaN  10327.643201       7.126653e+05        19.560179   \n",
       "\n",
       "       median_distance_km  external_driver_orders  internal_driver_orders  \\\n",
       "count           19.000000               19.000000               19.000000   \n",
       "mean            45.894737             1304.315789             5615.684211   \n",
       "min              1.000000                0.000000                0.000000   \n",
       "25%             35.500000                0.000000              388.500000   \n",
       "50%             48.000000               81.000000             1612.000000   \n",
       "75%             58.000000             1221.000000             6275.500000   \n",
       "max             83.000000             8578.000000            23373.000000   \n",
       "std             20.736159             2439.802680             8282.104584   \n",
       "\n",
       "       pickup_orders  delivery_orders    year  month  quarter  \n",
       "count      19.000000        19.000000    19.0   19.0     19.0  \n",
       "mean      909.736842      6256.473684  2025.0    6.0      2.0  \n",
       "min         0.000000         5.000000  2025.0    6.0      2.0  \n",
       "25%         0.000000       637.000000  2025.0    6.0      2.0  \n",
       "50%         4.000000      1589.000000  2025.0    6.0      2.0  \n",
       "75%      1371.000000      5483.500000  2025.0    6.0      2.0  \n",
       "max      5344.000000     29826.000000  2025.0    6.0      2.0  \n",
       "std      1496.157963      9236.427751     0.0    0.0      0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display sample data\n",
    "print(\"\\n=== Sample Data ===\")\n",
    "display(df.head(10))\n",
    "\n",
    "print(\"\\n=== Data Types ===\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\n=== Summary Statistics ===\")\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Time Series Decomposition\n",
    "\n",
    "Decompose time series into:\n",
    "- **Trend**: Long-term increase/decrease\n",
    "- **Seasonal**: Repeating patterns (yearly, quarterly, monthly)\n",
    "- **Residual**: Random noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data appears to already be at company level (no branch column)\n",
      "\n",
      "Total company time series: 19 months\n",
      "From 2025-06-01 00:00:00 to 2025-06-01 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Aggregate to total company level for decomposition\n",
    "# Check if data is already at company level or needs aggregation\n",
    "\n",
    "# Identify branch column\n",
    "branch_col = None\n",
    "for possible_name in ['Niederlassung', 'branch', 'Branch', 'branch_name']:\n",
    "    if possible_name in df.columns:\n",
    "        branch_col = possible_name\n",
    "        break\n",
    "\n",
    "# Define target columns (check which exist)\n",
    "target_cols_to_agg = {}\n",
    "if 'total_orders' in df.columns:\n",
    "    target_cols_to_agg['total_orders'] = 'sum'\n",
    "if 'external_driver_orders' in df.columns:\n",
    "    target_cols_to_agg['external_driver_orders'] = 'sum'\n",
    "if 'internal_driver_orders' in df.columns:\n",
    "    target_cols_to_agg['internal_driver_orders'] = 'sum'\n",
    "if 'total_distance_km' in df.columns:\n",
    "    target_cols_to_agg['total_distance_km'] = 'sum'\n",
    "\n",
    "if branch_col and len(target_cols_to_agg) > 0:\n",
    "    # Data has branches, aggregate to company level\n",
    "    df_total = df.groupby('year_month').agg(target_cols_to_agg).reset_index()\n",
    "    print(f\"Aggregated from {df[branch_col].nunique()} branches to company level\")\n",
    "elif 'year_month' in df.columns and len(target_cols_to_agg) > 0:\n",
    "    # Data appears to already be at company level\n",
    "    df_total = df.copy()\n",
    "    print(\"Data appears to already be at company level (no branch column)\")\n",
    "else:\n",
    "    # Fallback - use all numeric columns\n",
    "    print(\"⚠️  Using fallback aggregation\")\n",
    "    df_total = df.copy()\n",
    "\n",
    "# Sort by date\n",
    "if 'year_month' in df_total.columns:\n",
    "    df_total = df_total.sort_values('year_month').reset_index(drop=True)\n",
    "    print(f\"\\nTotal company time series: {len(df_total)} months\")\n",
    "    print(f\"From {df_total['year_month'].min()} to {df_total['year_month'].max()}\")\n",
    "else:\n",
    "    print(\"⚠️  Cannot sort - no year_month column\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Need at least 24 months for seasonal decomposition. Current: 19 months\n"
     ]
    }
   ],
   "source": [
    "# Seasonal decomposition for total orders\n",
    "# Using additive model (better for stable variance)\n",
    "# Period = 12 months for yearly seasonality\n",
    "\n",
    "if len(df_total) >= 24:  # Need at least 2 years for good decomposition\n",
    "    decomposition = seasonal_decompose(\n",
    "        df_total.set_index('year_month')['total_orders'],\n",
    "        model='additive',\n",
    "        period=12,\n",
    "        extrapolate_trend='freq'\n",
    "    )\n",
    "    \n",
    "    # Plot decomposition\n",
    "    fig, axes = plt.subplots(4, 1, figsize=(15, 10))\n",
    "    \n",
    "    decomposition.observed.plot(ax=axes[0], title='Observed')\n",
    "    axes[0].set_ylabel('Orders')\n",
    "    \n",
    "    decomposition.trend.plot(ax=axes[1], title='Trend')\n",
    "    axes[1].set_ylabel('Orders')\n",
    "    \n",
    "    decomposition.seasonal.plot(ax=axes[2], title='Seasonal')\n",
    "    axes[2].set_ylabel('Orders')\n",
    "    \n",
    "    decomposition.resid.plot(ax=axes[3], title='Residual')\n",
    "    axes[3].set_ylabel('Orders')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../results/seasonal_decomposition_total_orders.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate strength of trend and seasonality\n",
    "    var_resid = decomposition.resid.var()\n",
    "    var_trend = decomposition.trend.var()\n",
    "    var_seasonal = decomposition.seasonal.var()\n",
    "    \n",
    "    trend_strength = max(0, 1 - var_resid / (var_resid + var_trend))\n",
    "    seasonal_strength = max(0, 1 - var_resid / (var_resid + var_seasonal))\n",
    "    \n",
    "    print(\"\\n=== Decomposition Analysis ===\")\n",
    "    print(f\"Trend strength: {trend_strength:.2%}\")\n",
    "    print(f\"Seasonal strength: {seasonal_strength:.2%}\")\n",
    "    print(f\"\\nInterpretation:\")\n",
    "    print(f\"  - Strong trend (>70%): {'Yes' if trend_strength > 0.7 else 'No'}\")\n",
    "    print(f\"  - Strong seasonality (>70%): {'Yes' if seasonal_strength > 0.7 else 'No'}\")\n",
    "else:\n",
    "    print(f\"⚠️  Need at least 24 months for seasonal decomposition. Current: {len(df_total)} months\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Autocorrelation Analysis\n",
    "\n",
    "Identify:\n",
    "- **ACF (Autocorrelation)**: How current values relate to past values\n",
    "- **PACF (Partial Autocorrelation)**: Direct relationship after removing intermediate correlations\n",
    "- Useful for SARIMAX order selection (p, d, q parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ACF and PACF plots\n# Only run if we have enough data points\n\nif len(df_total) >= 10:  # Need at least 10 data points for meaningful ACF/PACF\n    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n    \n    # Limit lags to at most len(data) - 2\n    max_lags = min(24, len(df_total) - 2)\n    \n    # ACF\n    plot_acf(df_total['total_orders'].dropna(), lags=max_lags, ax=axes[0])\n    axes[0].set_title('Autocorrelation Function (ACF)')\n    axes[0].set_xlabel('Lag (months)')\n    \n    # PACF\n    plot_pacf(df_total['total_orders'].dropna(), lags=max_lags, ax=axes[1])\n    axes[1].set_title('Partial Autocorrelation Function (PACF)')\n    axes[1].set_xlabel('Lag (months)')\n    \n    plt.tight_layout()\n    plt.savefig('../results/acf_pacf_analysis.png', dpi=300, bbox_inches='tight')\n    plt.show()\n    \n    print(\"\\n=== ACF/PACF Interpretation Guide ===\")\n    print(\"For SARIMAX(p,d,q)(P,D,Q)[s] parameter selection:\")\n    print(\"  - ACF shows significant spikes at seasonal lags (12, 24) → Q parameter\")\n    print(\"  - PACF shows significant spikes at seasonal lags → P parameter\")\n    print(\"  - ACF decays gradually → MA component (q parameter)\")\n    print(\"  - PACF cuts off sharply → AR component (p parameter)\")\nelse:\n    print(f\"⚠️  Skipping ACF/PACF analysis - insufficient data ({len(df_total)} time periods)\")\n    print(f\"    Time series analysis requires multiple time periods (months/weeks)\")\n    print(f\"    Current data appears to be from a single time period with multiple groups (branches)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Seasonality Patterns by Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract month and year for analysis\n",
    "df_total['month'] = df_total['year_month'].dt.month\n",
    "df_total['year'] = df_total['year_month'].dt.year\n",
    "\n",
    "# Monthly patterns\n",
    "monthly_avg = df_total.groupby('month')['total_orders'].agg(['mean', 'std', 'min', 'max']).reset_index()\n",
    "monthly_avg['cv'] = monthly_avg['std'] / monthly_avg['mean']  # Coefficient of variation\n",
    "\n",
    "# Create month names\n",
    "month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "monthly_avg['month_name'] = monthly_avg['month'].apply(lambda x: month_names[x-1])\n",
    "\n",
    "print(\"\\n=== Monthly Seasonality Patterns ===\")\n",
    "display(monthly_avg)\n",
    "\n",
    "# Visualize monthly patterns\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=monthly_avg['month_name'],\n",
    "    y=monthly_avg['mean'],\n",
    "    error_y=dict(type='data', array=monthly_avg['std']),\n",
    "    name='Average Orders',\n",
    "    marker_color='lightblue'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Average Monthly Orders (±1 Std Dev)',\n",
    "    xaxis_title='Month',\n",
    "    yaxis_title='Total Orders',\n",
    "    height=500,\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.write_html('../results/monthly_seasonality.html')\n",
    "fig.show()\n",
    "\n",
    "# Identify peak and low months\n",
    "peak_month = monthly_avg.loc[monthly_avg['mean'].idxmax(), 'month_name']\n",
    "low_month = monthly_avg.loc[monthly_avg['mean'].idxmin(), 'month_name']\n",
    "peak_value = monthly_avg['mean'].max()\n",
    "low_value = monthly_avg['mean'].min()\n",
    "seasonality_amplitude = ((peak_value - low_value) / monthly_avg['mean'].mean()) * 100\n",
    "\n",
    "print(f\"\\n📊 Peak month: {peak_month} ({peak_value:.0f} orders)\")\n",
    "print(f\"📉 Low month: {low_month} ({low_value:.0f} orders)\")\n",
    "print(f\"📈 Seasonality amplitude: {seasonality_amplitude:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Year-over-Year Growth Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate year-over-year growth\n",
    "yearly_totals = df_total.groupby('year').agg({\n",
    "    'total_orders': 'sum',\n",
    "    'external_driver_orders': 'sum',\n",
    "    'internal_driver_orders': 'sum',\n",
    "    'total_distance_km': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "yearly_totals['yoy_growth'] = yearly_totals['total_orders'].pct_change() * 100\n",
    "yearly_totals['external_pct'] = (yearly_totals['external_driver_orders'] / yearly_totals['total_orders']) * 100\n",
    "yearly_totals['internal_pct'] = (yearly_totals['internal_driver_orders'] / yearly_totals['total_orders']) * 100\n",
    "\n",
    "print(\"\\n=== Year-over-Year Analysis ===\")\n",
    "display(yearly_totals)\n",
    "\n",
    "# Visualize yearly trends\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=('Total Orders by Year', 'YoY Growth Rate',\n",
    "                   'External vs Internal Split', 'Total Distance')\n",
    ")\n",
    "\n",
    "# Total orders\n",
    "fig.add_trace(\n",
    "    go.Bar(x=yearly_totals['year'], y=yearly_totals['total_orders'],\n",
    "           marker_color='steelblue', name='Orders'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# YoY growth\n",
    "fig.add_trace(\n",
    "    go.Bar(x=yearly_totals['year'][1:], y=yearly_totals['yoy_growth'][1:],\n",
    "           marker_color='coral', name='Growth %'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# External vs Internal\n",
    "fig.add_trace(\n",
    "    go.Bar(x=yearly_totals['year'], y=yearly_totals['external_pct'],\n",
    "           name='External %', marker_color='lightcoral'),\n",
    "    row=2, col=1\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Bar(x=yearly_totals['year'], y=yearly_totals['internal_pct'],\n",
    "           name='Internal %', marker_color='lightgreen'),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Total distance\n",
    "fig.add_trace(\n",
    "    go.Bar(x=yearly_totals['year'], y=yearly_totals['total_distance_km'],\n",
    "           marker_color='mediumpurple', name='Distance'),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(height=800, showlegend=True, title_text=\"Yearly Trends Analysis\")\n",
    "fig.update_xaxes(title_text=\"Year\", row=2, col=1)\n",
    "fig.update_xaxes(title_text=\"Year\", row=2, col=2)\n",
    "fig.update_yaxes(title_text=\"Orders\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Growth %\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Percentage\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Distance (km)\", row=2, col=2)\n",
    "\n",
    "fig.write_html('../results/yearly_trends_analysis.html')\n",
    "fig.show()\n",
    "\n",
    "# Calculate average growth rate\n",
    "avg_growth = yearly_totals['yoy_growth'].mean()\n",
    "print(f\"\\n📈 Average YoY growth rate: {avg_growth:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Branch-Level Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Branch-Level Correlation Analysis\n",
    "# Only run if we have branch-level data\n",
    "\n",
    "# Find branch column\n",
    "branch_col = None\n",
    "for possible_name in ['Niederlassung', 'branch', 'Branch', 'branch_name']:\n",
    "    if possible_name in df.columns:\n",
    "        branch_col = possible_name\n",
    "        break\n",
    "\n",
    "if branch_col and 'total_orders' in df.columns:\n",
    "    # Get top branches by total volume\n",
    "    top_branches = df.groupby(branch_col)['total_orders'].sum().nlargest(10).index.tolist()\n",
    "    \n",
    "    # Create pivot table for correlation\n",
    "    df_pivot = df[df[branch_col].isin(top_branches)].pivot(\n",
    "        index='year_month',\n",
    "        columns=branch_col,\n",
    "        values='total_orders'\n",
    "    )\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    correlation_matrix = df_pivot.corr()\n",
    "    \n",
    "    print(\"\\n=== Branch Correlation Matrix (Top 10 Branches) ===\")\n",
    "    print(\"High correlation (>0.8) indicates similar patterns\")\n",
    "    print(\"Low correlation (<0.3) indicates independent patterns\\n\")\n",
    "    \n",
    "    # Visualize correlation matrix\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "                center=0, vmin=-1, vmax=1, square=True, linewidths=0.5)\n",
    "    plt.title('Branch Order Correlation Matrix (Top 10 Branches)', fontsize=14, pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../results/branch_correlation_matrix.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Find highly correlated branch pairs\n",
    "    high_corr_threshold = 0.8\n",
    "    high_corr_pairs = []\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            if correlation_matrix.iloc[i, j] > high_corr_threshold:\n",
    "                high_corr_pairs.append((\n",
    "                    correlation_matrix.columns[i],\n",
    "                    correlation_matrix.columns[j],\n",
    "                    correlation_matrix.iloc[i, j]\n",
    "                ))\n",
    "    \n",
    "    if high_corr_pairs:\n",
    "        print(f\"\\n🔗 Highly correlated branch pairs (correlation > {high_corr_threshold}):\")\n",
    "        for branch1, branch2, corr in sorted(high_corr_pairs, key=lambda x: x[2], reverse=True):\n",
    "            print(f\"  {branch1} ↔ {branch2}: {corr:.3f}\")\n",
    "    else:\n",
    "        print(f\"\\nNo branch pairs with correlation > {high_corr_threshold}\")\n",
    "else:\n",
    "    print(\"⚠️  Skipping branch correlation analysis - no branch column found in data\")\n",
    "    print(\"    Data appears to already be aggregated at company level\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Target Variable Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between target variables\n",
    "target_cols = ['total_orders', 'external_driver_orders', 'internal_driver_orders', 'total_distance_km']\n",
    "available_targets = [col for col in target_cols if col in df_total.columns]\n",
    "\n",
    "if len(available_targets) > 1:\n",
    "    target_correlation = df_total[available_targets].corr()\n",
    "    \n",
    "    print(\"\\n=== Target Variable Correlations ===\")\n",
    "    display(target_correlation)\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(target_correlation, annot=True, fmt='.3f', cmap='RdYlGn',\n",
    "                center=0, vmin=-1, vmax=1, square=True, linewidths=1)\n",
    "    plt.title('Target Variable Correlation Matrix', fontsize=14, pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../results/target_correlation_matrix.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Pairplot for visual inspection\n",
    "    if len(df_total) < 500:  # Only if not too many data points\n",
    "        pairplot_data = df_total[available_targets].copy()\n",
    "        fig = px.scatter_matrix(\n",
    "            pairplot_data,\n",
    "            dimensions=available_targets,\n",
    "            title='Target Variable Pairplot',\n",
    "            height=800\n",
    "        )\n",
    "        fig.update_traces(diagonal_visible=False, showupperhalf=False)\n",
    "        fig.write_html('../results/target_pairplot.html')\n",
    "        fig.show()\n",
    "else:\n",
    "    print(\"Not enough target variables for correlation analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Stationarity Test (Augmented Dickey-Fuller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "def adf_test(series, name=''):\n",
    "    \"\"\"\n",
    "    Perform Augmented Dickey-Fuller test for stationarity\n",
    "    \n",
    "    Null hypothesis: Series has a unit root (non-stationary)\n",
    "    If p-value < 0.05: Reject null hypothesis (stationary)\n",
    "    \"\"\"\n",
    "    result = adfuller(series.dropna(), autolag='AIC')\n",
    "    \n",
    "    print(f\"\\n=== ADF Test Results: {name} ===\")\n",
    "    print(f\"ADF Statistic: {result[0]:.4f}\")\n",
    "    print(f\"p-value: {result[1]:.4f}\")\n",
    "    print(f\"Critical Values:\")\n",
    "    for key, value in result[4].items():\n",
    "        print(f\"  {key}: {value:.3f}\")\n",
    "    \n",
    "    if result[1] < 0.05:\n",
    "        print(f\"\\n✅ Result: STATIONARY (p-value < 0.05)\")\n",
    "        print(\"   → Can use without differencing\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"\\n❌ Result: NON-STATIONARY (p-value >= 0.05)\")\n",
    "        print(\"   → Need differencing (d=1 in SARIMAX)\")\n",
    "        return False\n",
    "\n",
    "# Test original series\n",
    "is_stationary = adf_test(df_total['total_orders'], 'Total Orders (Original)')\n",
    "\n",
    "# Test first difference if non-stationary\n",
    "if not is_stationary:\n",
    "    df_total['total_orders_diff'] = df_total['total_orders'].diff()\n",
    "    adf_test(df_total['total_orders_diff'], 'Total Orders (First Difference)')\n",
    "    \n",
    "    # Visualize original vs differenced\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(15, 8))\n",
    "    \n",
    "    axes[0].plot(df_total['year_month'], df_total['total_orders'])\n",
    "    axes[0].set_title('Original Series')\n",
    "    axes[0].set_ylabel('Total Orders')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1].plot(df_total['year_month'][1:], df_total['total_orders_diff'][1:])\n",
    "    axes[1].set_title('First Difference (Δ orders)')\n",
    "    axes[1].set_ylabel('Change in Orders')\n",
    "    axes[1].set_xlabel('Date')\n",
    "    axes[1].axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../results/stationarity_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze distribution of target variables\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "target_vars = [\n",
    "    ('total_orders', 'Total Orders'),\n",
    "    ('external_driver_orders', 'External Driver Orders'),\n",
    "    ('internal_driver_orders', 'Internal Driver Orders'),\n",
    "    ('total_distance_km', 'Total Distance (km)')\n",
    "]\n",
    "\n",
    "for idx, (col, title) in enumerate(target_vars):\n",
    "    if col in df_total.columns:\n",
    "        row = idx // 2\n",
    "        col_idx = idx % 2\n",
    "        \n",
    "        data = df_total[col].dropna()\n",
    "        \n",
    "        # Histogram with KDE\n",
    "        axes[row, col_idx].hist(data, bins=20, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "        axes[row, col_idx].set_title(f'{title} Distribution')\n",
    "        axes[row, col_idx].set_xlabel(title)\n",
    "        axes[row, col_idx].set_ylabel('Frequency')\n",
    "        \n",
    "        # Add mean and median lines\n",
    "        mean_val = data.mean()\n",
    "        median_val = data.median()\n",
    "        axes[row, col_idx].axvline(mean_val, color='red', linestyle='--', label=f'Mean: {mean_val:.0f}')\n",
    "        axes[row, col_idx].axvline(median_val, color='green', linestyle='--', label=f'Median: {median_val:.0f}')\n",
    "        axes[row, col_idx].legend()\n",
    "        axes[row, col_idx].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Perform normality test\n",
    "        statistic, p_value = stats.shapiro(data)\n",
    "        is_normal = p_value > 0.05\n",
    "        normality_text = \"Normal\" if is_normal else \"Non-normal\"\n",
    "        axes[row, col_idx].text(0.95, 0.95, f'{normality_text}\\n(p={p_value:.3f})',\n",
    "                               transform=axes[row, col_idx].transAxes,\n",
    "                               verticalalignment='top', horizontalalignment='right',\n",
    "                               bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/distribution_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== Distribution Statistics ===\")\n",
    "for col, title in target_vars:\n",
    "    if col in df_total.columns:\n",
    "        data = df_total[col].dropna()\n",
    "        print(f\"\\n{title}:\")\n",
    "        print(f\"  Mean: {data.mean():.2f}\")\n",
    "        print(f\"  Median: {data.median():.2f}\")\n",
    "        print(f\"  Std Dev: {data.std():.2f}\")\n",
    "        print(f\"  Skewness: {data.skew():.2f}\")\n",
    "        print(f\"  Kurtosis: {data.kurtosis():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect outliers using IQR method\n",
    "def detect_outliers_iqr(series, multiplier=1.5):\n",
    "    \"\"\"\n",
    "    Detect outliers using Interquartile Range (IQR)\n",
    "    \n",
    "    multiplier=1.5: Standard outlier detection\n",
    "    multiplier=3.0: Extreme outlier detection\n",
    "    \"\"\"\n",
    "    Q1 = series.quantile(0.25)\n",
    "    Q3 = series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - multiplier * IQR\n",
    "    upper_bound = Q3 + multiplier * IQR\n",
    "    \n",
    "    outliers = series[(series < lower_bound) | (series > upper_bound)]\n",
    "    \n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "print(\"\\n=== Outlier Detection (IQR Method) ===\")\n",
    "\n",
    "for col, title in target_vars:\n",
    "    if col in df_total.columns:\n",
    "        data = df_total[col].dropna()\n",
    "        outliers, lower, upper = detect_outliers_iqr(data)\n",
    "        \n",
    "        print(f\"\\n{title}:\")\n",
    "        print(f\"  Normal range: [{lower:.0f}, {upper:.0f}]\")\n",
    "        print(f\"  Outliers detected: {len(outliers)}\")\n",
    "        \n",
    "        if len(outliers) > 0:\n",
    "            print(f\"  Outlier values: {outliers.values}\")\n",
    "            outlier_dates = df_total.loc[outliers.index, 'year_month'].values\n",
    "            print(f\"  Outlier dates: {outlier_dates}\")\n",
    "\n",
    "# Visualize outliers\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "for idx, (col, title) in enumerate(target_vars):\n",
    "    if col in df_total.columns:\n",
    "        row = idx // 2\n",
    "        col_idx = idx % 2\n",
    "        \n",
    "        data = df_total[col].dropna()\n",
    "        outliers, lower, upper = detect_outliers_iqr(data)\n",
    "        \n",
    "        # Box plot\n",
    "        bp = axes[row, col_idx].boxplot([data], labels=[title], patch_artist=True)\n",
    "        bp['boxes'][0].set_facecolor('lightblue')\n",
    "        \n",
    "        axes[row, col_idx].set_title(f'{title} - Box Plot')\n",
    "        axes[row, col_idx].set_ylabel('Value')\n",
    "        axes[row, col_idx].grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Add outlier count\n",
    "        axes[row, col_idx].text(1.3, data.median(), f'{len(outliers)} outliers',\n",
    "                               verticalalignment='center',\n",
    "                               bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/outlier_detection.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Generate EDA Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive EDA summary\n",
    "\n",
    "# Find branch column\n",
    "branch_col = None\n",
    "for possible_name in ['Niederlassung', 'branch', 'Branch', 'branch_name']:\n",
    "    if possible_name in df.columns:\n",
    "        branch_col = possible_name\n",
    "        break\n",
    "\n",
    "num_branches = int(df[branch_col].nunique()) if branch_col else 1\n",
    "\n",
    "eda_summary = {\n",
    "    'analysis_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'data_summary': {\n",
    "        'total_months': int(len(df_total)),\n",
    "        'date_range': f\"{df_total['year_month'].min()} to {df_total['year_month'].max()}\",\n",
    "        'num_branches': num_branches\n",
    "    },\n",
    "    'seasonality': {\n",
    "        'seasonal_strength': float(seasonal_strength) if len(df_total) >= 24 else None,\n",
    "        'trend_strength': float(trend_strength) if len(df_total) >= 24 else None,\n",
    "        'peak_month': peak_month,\n",
    "        'low_month': low_month,\n",
    "        'seasonality_amplitude_pct': float(seasonality_amplitude)\n",
    "    },\n",
    "    'growth': {\n",
    "        'avg_yoy_growth_pct': float(avg_growth) if not np.isnan(avg_growth) else None,\n",
    "        'yearly_data': yearly_totals.to_dict('records')\n",
    "    },\n",
    "    'stationarity': {\n",
    "        'is_stationary': bool(is_stationary),\n",
    "        'recommended_differencing': 0 if is_stationary else 1\n",
    "    },\n",
    "    'model_recommendations': {}\n",
    "}\n",
    "\n",
    "# Add model recommendations based on findings\n",
    "recommendations = []\n",
    "\n",
    "if len(df_total) >= 24:\n",
    "    if seasonal_strength > 0.7:\n",
    "        recommendations.append(\"Strong seasonality detected → Prophet and SARIMAX recommended\")\n",
    "        recommendations.append(\"Use seasonal order (P,D,Q)[12] in SARIMAX\")\n",
    "    else:\n",
    "        recommendations.append(\"Weak seasonality → Consider simpler models first\")\n",
    "else:\n",
    "    recommendations.append(f\"Limited data ({len(df_total)} months) → Use simpler models, avoid complex seasonal models\")\n",
    "\n",
    "if not is_stationary:\n",
    "    recommendations.append(\"Non-stationary series → Use d=1 in SARIMAX\")\n",
    "else:\n",
    "    recommendations.append(\"Stationary series → Can use d=0 in SARIMAX\")\n",
    "\n",
    "if not np.isnan(avg_growth) and abs(avg_growth) > 5:\n",
    "    recommendations.append(f\"Strong growth trend ({avg_growth:.1f}%) → Consider linear trend component\")\n",
    "else:\n",
    "    recommendations.append(\"Stable growth → Standard trend handling sufficient\")\n",
    "\n",
    "eda_summary['model_recommendations']['insights'] = recommendations\n",
    "\n",
    "# Save summary to JSON\n",
    "with open('../results/eda_summary.json', 'w') as f:\n",
    "    # Convert numpy types to native Python types for JSON serialization\n",
    "    def convert_types(obj):\n",
    "        if isinstance(obj, dict):\n",
    "            return {k: convert_types(v) for k, v in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [convert_types(item) for item in obj]\n",
    "        elif isinstance(obj, (np.int64, np.int32)):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, (np.float64, np.float32)):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif pd.isna(obj):\n",
    "            return None\n",
    "        else:\n",
    "            return obj\n",
    "    \n",
    "    eda_summary_clean = convert_types(eda_summary)\n",
    "    json.dump(eda_summary_clean, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"📊 EXPLORATORY DATA ANALYSIS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n📅 Data Coverage:\")\n",
    "print(f\"   {eda_summary['data_summary']['total_months']} months\")\n",
    "print(f\"   {eda_summary['data_summary']['date_range']}\")\n",
    "print(f\"   {eda_summary['data_summary']['num_branches']} branches\")\n",
    "\n",
    "if eda_summary['seasonality']['seasonal_strength']:\n",
    "    print(f\"\\n📈 Seasonality:\")\n",
    "    print(f\"   Strength: {eda_summary['seasonality']['seasonal_strength']:.1%}\")\n",
    "    print(f\"   Peak month: {eda_summary['seasonality']['peak_month']}\")\n",
    "    print(f\"   Low month: {eda_summary['seasonality']['low_month']}\")\n",
    "    print(f\"   Amplitude: {eda_summary['seasonality']['seasonality_amplitude_pct']:.1f}%\")\n",
    "\n",
    "if eda_summary['growth']['avg_yoy_growth_pct']:\n",
    "    print(f\"\\n📊 Growth:\")\n",
    "    print(f\"   Average YoY: {eda_summary['growth']['avg_yoy_growth_pct']:.2f}%\")\n",
    "\n",
    "print(f\"\\n🔍 Stationarity:\")\n",
    "print(f\"   {'Stationary' if eda_summary['stationarity']['is_stationary'] else 'Non-stationary'}\")\n",
    "print(f\"   Recommended differencing: d={eda_summary['stationarity']['recommended_differencing']}\")\n",
    "\n",
    "print(f\"\\n💡 Model Recommendations:\")\n",
    "for rec in recommendations:\n",
    "    print(f\"   • {rec}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✅ EDA Complete!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n📁 Saved:\")\n",
    "print(\"   - results/eda_summary.json\")\n",
    "if len(df_total) >= 24:\n",
    "    print(\"   - results/seasonal_decomposition_total_orders.png\")\n",
    "print(\"   - results/acf_pacf_analysis.png\")\n",
    "print(\"   - results/monthly_seasonality.html\")\n",
    "print(\"   - results/yearly_trends_analysis.html\")\n",
    "if branch_col:\n",
    "    print(\"   - results/branch_correlation_matrix.png\")\n",
    "print(\"   - results/target_correlation_matrix.png\")\n",
    "if len(df_total) < 500:\n",
    "    print(\"   - results/target_pairplot.html\")\n",
    "print(\"   - results/distribution_analysis.png\")\n",
    "print(\"   - results/outlier_detection.png\")\n",
    "print(\"\\n➡️  NEXT STEPS:\")\n",
    "print(\"   Proceed to notebook 06: Baseline Models (Prophet)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}